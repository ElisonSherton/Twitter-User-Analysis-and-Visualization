{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter User Analysis\n",
    "\n",
    "This project aims to understand the behavior of a user through their tweets and interaction with other users on a popular social media platform called [twitter](https://twitter.com). I reached out to a friend of mine [Shriram Dusane](https://twitter.com/shriramdusane) and he nodded his consent for the data to be used for the sake of this analysis.\n",
    "\n",
    "I scraped the tweets using this tool from github called [twint](https://github.com/twintproject/twint) which gave me all the data I needed for the sake of this analysis. You can look up the link attached above in order to explore the tool but for those who're short on patience, you can open up your anaconda prompt and type in the following commands to extract a user's info using twint.\n",
    "```\n",
    "pip3 install twint\n",
    "twint -u username -o file.json --json\n",
    "```\n",
    "\n",
    "In place of the username, type in the user's name i.e. @shriramdusane or @xyz and in place of file, type in the name of the file you want to save the tweets as. The file will be stored in the destination from where you're running the anaconda prompt.\n",
    "\n",
    "Post getting the tweets, we need to clean and organize the data in a particular way for further analysis in tableau. Let's get straight to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "\n",
    "# For dataframe related opns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# To read tweets from the json file\n",
    "import json\n",
    "\n",
    "# To view random slices of data from the dataframe\n",
    "import random\n",
    "\n",
    "# To generate a wordcloud out of hashtags\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# To read images\n",
    "from PIL import Image\n",
    "\n",
    "# For counting occurences of hashtags\n",
    "from collections import Counter\n",
    "\n",
    "# For handling text and preprocessing it\n",
    "import re\n",
    "import regex\n",
    "import emoji\n",
    "\n",
    "# To supress unnecessary warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# To view/display complete data inline instead of truncated data\n",
    "pd.set_option(\"display.max_colwidth\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = './all_tweets.json'\n",
    "x = []\n",
    "with open(f, encoding = 'utf-8') as f:\n",
    "    x.append(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 1220042301159833605, 'conversation_id': '1219976785598861312', 'created_at': 1579715726000, 'date': '2020-01-22', 'time': '23:25:26', 'timezone': 'India Standard Time', 'user_id': 1587976986, 'username': 'shriramdusane', 'name': 'Shriram Dusane', 'place': '', 'tweet': '@YugaBhat makes extremely delicious pasta üçù', 'mentions': ['kushalvala', 'yugabhat'], 'urls': [], 'photos': [], 'replies_count': 1, 'retweets_count': 0, 'likes_count': 3, 'hashtags': [], 'cashtags': [], 'link': 'https://twitter.com/shriramdusane/status/1220042301159833605', 'retweet': False, 'quote_url': '', 'video': 0, 'near': '', 'geo': '', 'source': '', 'user_rt_id': '', 'user_rt': '', 'retweet_id': '', 'reply_to': [{'user_id': '1587976986', 'username': 'shriramdusane'}, {'user_id': '1012809324', 'username': 'kushalvala'}, {'user_id': '168484220', 'username': 'YugaBhat'}], 'retweet_date': '', 'translate': '', 'trans_src': '', 'trans_dest': ''}\n"
     ]
    }
   ],
   "source": [
    "# View one tweet item from the json file\n",
    "print(json.loads(x[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read data from json elements and structure them to be converted into a dataframe\n",
    "records = []\n",
    "for i in x[0]:\n",
    "    record = {}\n",
    "    data= (json.loads(i))\n",
    "    record['Date'] =pd.to_datetime(data['date'] + ' ' + data['time'])\n",
    "    record['TZ'] = data['timezone']\n",
    "    # Since we're getting links in urls, remove them from text\n",
    "    record['Tweet'] = re.sub(r'http[s]?://\\S+', '', data['tweet'])\n",
    "    record['Tweet'] = re.sub(r'pic.twitter.com/\\w+','', record['Tweet'])\n",
    "    \n",
    "    record['Word_count'] = len(data['tweet'].split())\n",
    "    record['Character_count'] = len(data['tweet'])\n",
    "    record['Mentions'] = data['mentions']\n",
    "    record['Replies_Count'] = data['replies_count']\n",
    "    record['Retweets_Count'] = data['retweets_count']\n",
    "    record['Likes'] = data['likes_count']\n",
    "    record['Hashtags'] = data['hashtags']\n",
    "    record['Link'] = data['link']\n",
    "    record['photos'] = data['photos']\n",
    "    record['urls'] = data['urls']\n",
    "    records.append(record)\n",
    "data = pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/43146528/how-to-extract-all-the-emojis-from-text\n",
    "# Function to extract emojis from text\n",
    "def split_count(text):\n",
    "    emoji_list = []\n",
    "    data = regex.findall(r'\\X', text)\n",
    "    for word in data:\n",
    "        if any(char in emoji.UNICODE_EMOJI for char in word):\n",
    "            emoji_list.append(word)\n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove '#' sign from the hashtags\n",
    "def remove_hash_sign(items):\n",
    "    hashes = []\n",
    "    for i in items:\n",
    "        hashes.append(re.sub(r'#','', i))\n",
    "    return hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operate the functions defined above on the dataframe\n",
    "data['Emojis'] = data['Tweet'].apply(lambda x: split_count(x))\n",
    "data['Hashtags'] = data['Hashtags'].apply(lambda x: remove_hash_sign(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of emojis and create a field for the same\n",
    "# Save the data to a csv file\n",
    "data['Emoji_Count'] = data.Emojis.apply(lambda x: len(x))\n",
    "data.to_csv('Shrirams_Tweets.csv', header = True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>TZ</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Word_count</th>\n",
       "      <th>Character_count</th>\n",
       "      <th>Mentions</th>\n",
       "      <th>Replies_Count</th>\n",
       "      <th>Retweets_Count</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Link</th>\n",
       "      <th>photos</th>\n",
       "      <th>urls</th>\n",
       "      <th>Emojis</th>\n",
       "      <th>Emoji_Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1055</td>\n",
       "      <td>2019-04-30 22:17:31</td>\n",
       "      <td>India Standard Time</td>\n",
       "      <td>Thanks for eating a piece of cake when I offered.</td>\n",
       "      <td>10</td>\n",
       "      <td>49</td>\n",
       "      <td>[anantikamehra, laldabba]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/shriramdusane/status/1123267648303722496</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7460</td>\n",
       "      <td>2014-06-23 22:27:08</td>\n",
       "      <td>India Standard Time</td>\n",
       "      <td>@brokntransistor yea man.. I always fuck up without the tuner üòê</td>\n",
       "      <td>11</td>\n",
       "      <td>63</td>\n",
       "      <td>[brokntransistor]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/shriramdusane/status/481118770430504960</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[üòê]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>761</td>\n",
       "      <td>2019-09-01 00:17:19</td>\n",
       "      <td>India Standard Time</td>\n",
       "      <td>Ae nahi yaar! Mazaa aa raha hai üëåüèªüòÇ</td>\n",
       "      <td>8</td>\n",
       "      <td>35</td>\n",
       "      <td>[sassthree]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/shriramdusane/status/1167871504542945280</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[üëåüèª, üòÇ]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3610</td>\n",
       "      <td>2017-08-21 21:32:54</td>\n",
       "      <td>India Standard Time</td>\n",
       "      <td>Thanks üòõ</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>[pskylarke, astikkulkarni, youtube]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/shriramdusane/status/899663109006123008</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[üòõ]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2599</td>\n",
       "      <td>2018-01-28 00:08:10</td>\n",
       "      <td>India Standard Time</td>\n",
       "      <td>Your ability to miss the point is magnificently foregrounded in most of the discussions.</td>\n",
       "      <td>14</td>\n",
       "      <td>88</td>\n",
       "      <td>[imanuraagsharma, factsionary]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/shriramdusane/status/957321854359814144</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Date                   TZ  \\\n",
       "1055 2019-04-30 22:17:31  India Standard Time   \n",
       "7460 2014-06-23 22:27:08  India Standard Time   \n",
       "761  2019-09-01 00:17:19  India Standard Time   \n",
       "3610 2017-08-21 21:32:54  India Standard Time   \n",
       "2599 2018-01-28 00:08:10  India Standard Time   \n",
       "\n",
       "                                                                                         Tweet  \\\n",
       "1055  Thanks for eating a piece of cake when I offered.                                          \n",
       "7460  @brokntransistor yea man.. I always fuck up without the tuner üòê                            \n",
       "761   Ae nahi yaar! Mazaa aa raha hai üëåüèªüòÇ                                                        \n",
       "3610  Thanks üòõ                                                                                   \n",
       "2599  Your ability to miss the point is magnificently foregrounded in most of the discussions.   \n",
       "\n",
       "      Word_count  Character_count                             Mentions  \\\n",
       "1055  10          49               [anantikamehra, laldabba]             \n",
       "7460  11          63               [brokntransistor]                     \n",
       "761   8           35               [sassthree]                           \n",
       "3610  2           8                [pskylarke, astikkulkarni, youtube]   \n",
       "2599  14          88               [imanuraagsharma, factsionary]        \n",
       "\n",
       "      Replies_Count  Retweets_Count  Likes Hashtags  \\\n",
       "1055  0              0               1      []        \n",
       "7460  1              0               0      []        \n",
       "761   0              0               0      []        \n",
       "3610  0              0               0      []        \n",
       "2599  1              0               1      []        \n",
       "\n",
       "                                                              Link photos  \\\n",
       "1055  https://twitter.com/shriramdusane/status/1123267648303722496  []      \n",
       "7460  https://twitter.com/shriramdusane/status/481118770430504960   []      \n",
       "761   https://twitter.com/shriramdusane/status/1167871504542945280  []      \n",
       "3610  https://twitter.com/shriramdusane/status/899663109006123008   []      \n",
       "2599  https://twitter.com/shriramdusane/status/957321854359814144   []      \n",
       "\n",
       "     urls   Emojis  Emoji_Count  \n",
       "1055  []   []       0            \n",
       "7460  []   [üòê]      1            \n",
       "761   []   [üëåüèª, üòÇ]  2            \n",
       "3610  []   [üòõ]      1            \n",
       "2599  []   []       0            "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly subset 5 elements from the array\n",
    "elems = [random.choice(data.index) for i in range(5)]\n",
    "data.iloc[elems,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a wordcloud from the given image and text\n",
    "def create_word_cloud(string, year, pic_loc, dest, transparency = 0.5):\n",
    "    # Background on which to overlay the wordcloud\n",
    "    background = np.array(Image.open(pic_loc))\n",
    "    \n",
    "    # Generation of wordcloud, specify color, maximum number of words, stopwords etc.\n",
    "    cloud = WordCloud(background_color = \"white\", max_words = 200, mask = background, stopwords = set(STOPWORDS))\n",
    "    cloud.generate(string)\n",
    "    cloud.to_file(dest)\n",
    "    \n",
    "    # Read the recently created wordcloud image and make it ready for \n",
    "    # merging/overlaying\n",
    "    overlay = Image.open(dest)\n",
    "    background = Image.open(pic_loc).convert(\"RGBA\")\n",
    "    overlay = overlay.convert(\"RGBA\")\n",
    "\n",
    "    # Overlay the wordcloud on the background image and save it\n",
    "    new_img = Image.blend(overlay, background, transparency)\n",
    "    new_img.save(dest, \"PNG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of all years in order to iterate over\n",
    "years = data.Date.dt.year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a hashtag word cloud for every year\n",
    "all_hashes = {}\n",
    "for year in years:\n",
    "    hashtags = []\n",
    "    for idx in data.index:\n",
    "        if data.iloc[idx, 0].year == year:\n",
    "            for i in data.iloc[idx,9]:\n",
    "                hashtags.append(i)\n",
    "    all_hashes[year] = \" \".join(hashtags)\n",
    "    create_word_cloud(all_hashes[year], year, f\"shriram_{year}.jpg\", f\"{year}_Hashcloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and save all the hashtags yearwise in a csv file\n",
    "yearwise_hashtags = {}\n",
    "for year in years:\n",
    "    c = Counter()\n",
    "    for idx in data.index:\n",
    "        if data.iloc[idx, 0].year == year:\n",
    "            for i in data.iloc[idx,13]:\n",
    "                c[i] += 1\n",
    "    yearwise_hashtags[year] = c\n",
    "    \n",
    "records = []\n",
    "\n",
    "for year in years:\n",
    "    for k, v in dict(yearwise_hashtags[year]).items():\n",
    "        records.append([k, v, year])\n",
    "hashes = pd.DataFrame(records, columns = ['Hashtag', 'Count', 'Year'])       \n",
    "hashes.to_csv(\"Yearwise_Hashtags.csv\", header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and save all the mentions to a file (yearwise)\n",
    "yearwise_mentions = {}\n",
    "for year in years:\n",
    "    c = Counter()\n",
    "    for idx in data.index:\n",
    "        if data.iloc[idx, 0].year == year:\n",
    "            for i in data.iloc[idx,5]:\n",
    "                c[i] += 1\n",
    "    yearwise_mentions[year] = c\n",
    "    \n",
    "records = []\n",
    "\n",
    "for year in years:\n",
    "    for k, v in dict(yearwise_mentions[year]).items():\n",
    "        records.append([k, v, year, 'https://twitter.com/' + k])\n",
    "hashes = pd.DataFrame(records, columns = ['Mention', 'Count', 'Year', 'User URL'])       \n",
    "hashes.to_csv(\"Yearwise_Mentions.csv\", header = True, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
